//
//  SpeechRecognizer.swift
//  NeuroNote
//
//  Created by Ellie on 7/3/25.
//

import Foundation
import Speech
import SwiftData
@Observable
class SpeechRecognizer {
    var recognizedText: String = "No speech recognized"
    
    init() {
        requestAuthorization()
    }
    
    private func requestAuthorization() {
        SFSpeechRecognizer.requestAuthorization { authStatus in
            DispatchQueue.main.async {
                switch authStatus {
                case .authorized:
                    print("‚úÖ Speech recognition authorized")
                case .denied, .restricted, .notDetermined:
                    print("‚ùå Speech recognition not authorized")
                @unknown default:
                    fatalError("Unknown speech auth status")
                }
            }
        }
    }
    func transcribeAudioFile(at url: URL, for segment: TranscriptionSegment, in context: ModelContext) {
        print("üéôÔ∏è Starting transcription for: \(url.lastPathComponent)")
        Task {
            do {
                if false {
                    try await transcribeWithWhisper(url: url, segment: segment, context: context)
                } else {
                    try await transcribeWithApple(url: url, segment: segment, context: context)
                }
            } catch {
                await handleTranscriptionFailure(url: url, segment: segment, context: context)
            }
        }
    }
    
    private func transcribeWithWhisper(url: URL, segment: TranscriptionSegment, context: ModelContext) async throws {
        segment.status = .transcribing
        try? context.save()
        print("üì° Sending audio to Whisper API...")
        let text = try await WhisperService.transcribe(audioURL: url)
        segment.transcriptionText = text
        segment.status = .completed
        try? context.save()
        print("‚úÖ Whisper transcribed: \(text)")
    }
    
    private func transcribeWithApple(url: URL, segment: TranscriptionSegment, context: ModelContext) async throws {
        try await withCheckedThrowingContinuation { continuation in
            let recognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
            let request = SFSpeechURLRecognitionRequest(url: url)
            request.shouldReportPartialResults = false // ‚úÖ Important for finalized result

            var didFinish = false

            let task = recognizer?.recognitionTask(with: request) { result, error in
                guard !didFinish else { return }

                if let result = result, result.isFinal {
                    segment.transcriptionText = result.bestTranscription.formattedString
                    segment.status = .completed
                    try? context.save()
                    print("‚úÖ Apple STT transcribed: \(segment.transcriptionText ?? "")")
                    continuation.resume()
                    didFinish = true
                } else if let error = error {
                    continuation.resume(throwing: error)
                    didFinish = true
                }
            }

            // ‚è±Ô∏è Timeout fallback (sometimes no .isFinal is triggered for short clips)
            DispatchQueue.main.asyncAfter(deadline: .now() + 5) {
                if !didFinish {
                    print("‚è±Ô∏è Forcing task cancel due to no final result")
                    task?.cancel()
                    continuation.resume(throwing: NSError(domain: "STTTimeout", code: -1))
                    didFinish = true
                }
            }
        }
    }
    private func handleTranscriptionFailure(url: URL, segment: TranscriptionSegment, context: ModelContext) async {
        segment.attemptCount += 1
        print("‚ö†Ô∏è Transcription failed. Retrying \(segment.attemptCount)...")
        
        if segment.attemptCount >= 5 {
            if segment.useWhisper {
                print("‚ö†Ô∏è Switching to Apple STT fallback")
                segment.useWhisper = false
            } else {
                segment.status = .failed
                print("‚ùå Max transcription retries reached")
            }
        } else {
            let delay = pow(2.0, Double(segment.attemptCount))
            try? await Task.sleep(nanoseconds: UInt64(delay * 1_000_000_000))
            transcribeAudioFile(at: url, for: segment, in: context)
        }
        
        try? context.save()
    }
}
